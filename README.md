# **Gesture Robot Controller**

## **Project proposal**

### **Problem**

Traditionally, different robots are controlled by their specialized controller from different companies, and it takes time to learn how to smoothly and naturally use them.

![Visual Aid](https://github.com/0-EricZhou-0/GestureRobotController/blob/24163eae153ecba82c3c52a8171683d5026e511e/visual_aid.png)

### **Solution**

We plan to design a robot controller which can recognize human gestures and send corresponding commands to robots. The system consists of three major subsystems.

The first one is Human Positioning System, which is in charge of reading the sensors placed on the body of the user and calculating the position of different body parts of the user and broadcast through Bluetooth.

The second one is Gesture Controlling System. It will enable the gestures and movements of the user to be translated to actual robot controlling actions.

The third one is Robot Feedback System. The system is used to transmit the warnings of robots to signals that can be sensed by humans through actuators like lights (LEDs), vibration motors, and buzzers.

## **Design**

### **Block Diagram**

![Block Diagram](https://github.com/0-EricZhou-0/GestureRobotController/blob/main/block_diagram.png)

### **Subsystem Overview**

#### **Human Positioning System**

Multiple IMUs are deployed on the joints of the user to detect the relative positions, they will be used to read exact data (i.e. orientations in deg/rad). The system will output the positions of human body parts (e.g. legs, arms, fingers, etc.) after calculating using an embedded processor placed on the user. Then the board will broadcast the measured results through a wireless connection (Bluetooth). Although the result is mainly used to control the motion of a pre-built robot, the interface provided by the software should be general enough for other systems to easily utilize the data.

#### **Gesture Control System**

After these data are sent to a PC via a wireless connection. The movements of human body would be reconstructed and be used to recognize different gestures.

##### **Robot Feedback System**

When robots enter special mode or encounter unexpected events, they can give some feedback on our controller. These messages can be demonstrated by led, vibration, sound, or even a small screen.

### **Subsystem Requirements**

#### **Human Positioning System**

The system consists of a power source, a embedded processor, multiple IMUs, and a transceiver. Firstly, the IMUs are placed on user's body, and the user will input the locations of these sensors to the embedded processor. The embedded processor will constantly read data from the IMUs using I2C protocol, then it will calculate the position of these IMUs in the space. Together with user predefined IMU position, it will work out the positions and orientations of user's body parts. After that, the transceiver will output these raw data to PC through 2.45GHz Bluetooth data-link.

#### **Gesture Control System**

The system would be a software resides on the PC. It will take in the raw data transferred by transceiver of the Human Positioning System, and detect gestures using those data. Ideally, it can recognize many gestures such as motion of arm, motion of leg, making a fist, waving arms, shake head, etc. depending on the positions of the IMUs placed on the user. Then the system will generate controls based on these gestures, and send to the robot using another 2.45GHz Bluetooth data-link.

#### **Robot Feedback System**

The system would be a program resides on both PC and embedded processor of the controller. It will be used to transfer some signals generated by the robot back to the user through the actuators placed on user. It will also use the same 2.45GHz Bluetooth data-link that the controller used to communicate with PC, sending the signals to trigger vibration motors, LEDs, and buzzers on the controller.

### **Tolerance Analysis**

Gesture recognition algorithm is the most difficult part for us, since none of us has related experience on it. We currently only plan to recognize static gestures, but dynamic gestures clearly have more potential.

## **Ethics and Safety**

### User Safety

1. Since our product will directly contact human skin, it is important to separate skin from electricity and make sure that no hazardous components will put users in danger.

2. When power is low, the product should notify user and shutdown the system automatically to prevent battery from starvation. If the system is not in use in a long time, it should also shutdown by itself.

### Build Safety

1. Always power off the battery before any hardware changes. Rubber gloves are required before touching any electric components.
